---
title: "Lab_1"
author: "Shipeng Liu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
library(bnlearn)
library(gRain)
```


# (1) Show that multiple runs of the hill-climbing algorithm can return non-equivalent Bayesian network (BN) structures. Explain why this happens. Use the Asia dataset which is included in the bnlearn package. To load the data, run data("asia"). Recall from the lectures that the concept of non-equivalent BN structures has a precise meaning.

```{r}
set.seed(12345)

showDifference=function(g1,g2){
  cat("  Is two graph equivalent?",all.equal(cpdag(g1),cpdag(g2)))
  graphviz.compare(g1,g2)
  print(cpdag(g1))
  print(cpdag(g2))
}

data("asia")


# Different initial graph
dag_1=hc(asia)
init_dag=random.graph(names(asia),num=1,method = "ordered")
dag_2=hc(asia,start = init_dag)

showDifference(dag_1,dag_2)

```


```{r}
# Different iss
dag_3=hc(asia,score='bde',iss=1)
dag_4=hc(asia,score='bde',iss=2)

showDifference(dag_3,dag_4)
```

Given different initial graphs, or different iss using BDeu score, the learnt graphs are not equivalent(Have different adjacencies and unshielded colliders). It's because we choose diffierent initial hyperparameters and Hill-Climbing only promise to find the local minimum.

In the second instance,iss(imaginary sample size) is related to the prior $p(\theta_{x_i|Pa_i=j}|G)$, the latter follows Dirichlet distribution.As we know, the prior distribution will influence the posterior distribution.


# (2) Learn a BN from 80 % of the Asia dataset. The dataset is included in the bnlearn package. To load the data, run data("asia"). Learn both the structure and the parameters. Use any learning algorithm and settings that you consider appropriate. Use the BN learned to classify the remaining 20 % of the Asia dataset in two classes: S = yes and S = no. In other words, compute the posterior probability distribution of S for each case and classify it in the most likely class. To do so, you have to use exact or approximate inference with the help of the bnlearn and gRain packages, i.e. you are not allowed to use functions such as predict. Report the confusion matrix, i.e. true/false positives/negatives.

```{r}
asiaSample=sample(nrow(asia),floor(nrow(asia)*0.8))
trainSet=asia[asiaSample,]
testSet=asia[-asiaSample,]
trueDag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
dag=hc(trainSet,score='bde',iss=2)

graphviz.compare(dag,trueDag)

```

## compute the posterior probability distribution of S for each case and classify it in the most likely class

```{r,warning=FALSE}
bnInference=function(dag,trainSet,testSet,target,mb=0){
  fittedDag=bn.fit(dag,trainSet)
  grainDag=as.grain(fittedDag)
  compiledDag=compile(grainDag)
  
  if(mb==0){
    requiredNodes=setdiff(colnames(asia),target)
  }else{
    requiredNodes=mb(fittedDag,target)
  }
  
  predict=function(testInstence){
    evidence=setEvidence(compiledDag,nodes=requiredNodes,states=testInstence)
    return(ifelse(querygrain(evidence,nodes=target)[[1]][[1]]>0.5,"no","yes"))
  }

  res=sapply(as.data.frame(t(testSet[,requiredNodes])),predict)
  
  return(res)
}


# Fit the true dag
trueRes=bnInference(trueDag,trainSet,testSet,"S")
trueRes <- factor(trueRes, levels = c("yes", "no"))

# Fit the learnt dag
learntRes=bnInference(dag,trainSet,testSet,"S")
learntRes <- factor(learntRes, levels = c("yes", "no"))
```

## Confusion Matrix

### True Graph

```{r}
# Confusion matrix 
trueDagMatrix <- table(testSet$S, trueRes)
print(trueDagMatrix[,c(2,1)])

```

### Own Result

```{r}
# Confusion matrix 
dagMatrix <- table(testSet$S, learntRes)
print(dagMatrix[,c(2,1)])

```

We have the same result!


# (3) In the previous exercise, you classiﬁed the variable S given observations for all the rest of the variables. Now, you are asked to classify S given observations only for the so-called Markov blanket of S, i.e. its parents plus its children plus the parents of its children minus S itself. Report again the confusion matrix.

```{r warning= FALSE}
mbLearntRes=bnInference(dag,trainSet,testSet,"S",mb=1)
mbLearntRes <- factor(mbLearntRes, levels = c("yes", "no"))
mbDagMatrix <- table(testSet$S, mbLearntRes)
print(mbDagMatrix[,c(2,1)])

```

The confusion matrix is the same as we get in (2), which tells that the markov blanket of S(node L and B) gives all the causal information of S.


# (4) Repeat the exercise (2) using a naive Bayes classiﬁer, i.e. the predictive variables are independent given the class variable. See p. 380 in Bishop’s book or Wikipedia for more information on the naive Bayes classiﬁer. Model the naive Bayes classiﬁer as a BN. You have to create the BN by hand.

```{r}
naiveBN = model2network("[S][A|S][T|S][L|S][B|S][E|S][X|S][D|S]")
plot(naiveBN)
```

## Confusion Matrix

```{r warning=FALSE}
# Fit the naive bayes dag
nbRes=bnInference(naiveBN,trainSet,testSet,"S")
nbRes <- factor(nbRes, levels = c("yes", "no"))

# Confusion matrix 
nbMatrix <- table(testSet$S, nbRes)
print(nbMatrix[,c(2,1)])
```


# (5) Explain why you obtain the same or different results in the exercises (2-4)

## Why the same result in exercise (2)?

In the true graph, the variables A and T are not independent.

### For the DAG we learned in score based approach

$$\begin{aligned}
    P(S|Y\setminus S)&= \frac{P(Y)}{P(Y\setminus S)} \\
    &= \frac{\prod_{Y}P(X_i|Pa_i)}{\sum_S\prod_{Y\setminus S}P(X_i|Pa_i)} \\
    &= \frac{P(A)P(T)P(S)P(L|S)P(B|S)P(E|T,L)P(D|B,E)P(X|E)}{\tau_1(L)\tau_2(B)P(A)P(T)P(E|T,L)P(X|E)P(D|E,B)} \\
    &= \frac{P(S)P(L|S)P(B|S)}{\tau_1(L)\tau_2(B)}
\end{aligned}$$

where $Y$ is the set of all the variable in the directed acyclic graph, $\tau_1$,$\tau_2$ is the marginal distribution of L,B, we get them using Variable Elimination.

### For the true DAG

$$\begin{aligned}
    P(S|Y\setminus S)&= \frac{P(Y)}{P(Y\setminus S)} \\
    &= \frac{\prod_{Y}P(X_i|Pa_i)}{\sum_S\prod_{Y\setminus S}P(X_i|Pa_i)} \\
    &= \frac{P(A)P(T|A)P(S)P(L|S)P(B|S)P(E|T,L)P(D|B,E)P(X|E)}{\tau_1(L)\tau_2(B)P(A)P(T|A)P(E|T,L)P(X|E)P(D|E,B)} \\
    &= \frac{P(S)P(L|S)P(B|S)}{\tau_1(L)\tau_2(B)}
\end{aligned}$$

From the induction formula,the distribution of $S$ given $Y\setminus S$ only relate to the variables $B$,$L$ and $S$,Which is also the reason we get the same result in exercise 3.

## Why the same result in exercise (3)?

$$\begin{aligned}
    P(S|U\setminus S)&= \frac{P(U)}{P(U\setminus S)} \\
    &= \frac{P(S)P(L|S)P(B|S)}{\tau_1(L)\tau_2(B)} \\
\end{aligned}$$

Where $U$ is a subset of set $X$,and $U=\lbrace S,L,B\rbrace$ i.e.S and its markov blanket.

## Why different result in exercise (4)?

### In exercise 4 we use Naive Bayesan model to classify.

$$\begin{aligned}
    P(S|Y\setminus S)&= \frac{P(Y)}{P(Y\setminus S)} \\
    &= \frac{\prod_{Y}P(X_i|Pa_i)}{\sum_S\prod_{Y\setminus S}P(X_i|Pa_i)} \\
    &= \frac{P(S)P(A|S)P(T|S)P(L|S)P(B|S)P(E|S)P(D|S)P(X|S)}{\tau_1(A,P,L,B,E,D,X)}
\end{aligned}$$




